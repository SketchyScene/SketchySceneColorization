<!doctype html>
<html lang="en">


<!-- === Header Starts === -->
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Language-based Colorization of Scene Sketches</title>

  <link href="./assets/bootstrap.min.css" rel="stylesheet">
  <link href="./assets/font.css" rel="stylesheet" type="text/css">
  <link href="./assets/style.css" rel="stylesheet" type="text/css">
</head>
<!-- === Header Ends === -->


<body>


<!-- === Home Section Starts === -->
<div class="section">
  <!-- === Title Starts === -->
    <div class="title">
      <b>Language-based <font color=#d400ff>C</font><font color=#ef4470>o</font><font color=#26abe3>l</font><font color=#ffcc00>o</font><font color=#33cc33>r</font><font color=#d400ff>i</font><font color=#ef4470>z</font><font color=#26abe3>a</font><font color=#33cc33>t</font><font color=#ffcc00>i</font><font color=#ef4470>o</font><font color=#26abe3>n</font> of Scene Sketches</b>
    </div>
  <!-- === Title Ends === -->
  <div class="author">
    <a href="https://changqingzou.weebly.com/" target="_blank">Changqing Zou</a><sup>#1,2</sup>,&nbsp;
    <a href="http://mo-haoran.com/" target="_blank">Haoran Mo</a><sup>#1</sup>(joint first author),&nbsp;
    <a href="http://sdcs.sysu.edu.cn/content/2537" target="_blank">Chengying Gao</a><sup>*1</sup>,&nbsp;
    <a href="http://www.duruofei.com/" target="_blank">Ruofei Du</a><sup>3</sup>,&nbsp;
    <a href="http://sweb.cityu.edu.hk/hongbofu/" target="_blank">Hongbo Fu</a><sup>4</sup>
  </div>
  <div class="institution">
    Sun Yat-sen University<sup>1</sup>,&nbsp;
    Huawei Noah's Ark Lab<sup>2</sup>,&nbsp;
    <br>
    Google<sup>3</sup>,&nbsp;
    City University of Hong Kong<sup>4</sup>
  </div>
  <br>
  <div class="institution">
    Accepted by <a href="https://sa2019.siggraph.org/" target="_blank">SIGGRAPH Asia 2019</a>
  </div>
  <div class="link">
    <a href="http://mo-haoran.com/files/SIGA19/SketchColorization_paper_SA2019.pdf" target="_blank">[Paper]</a>&nbsp;
    <a href="https://github.com/SketchyScene/SketchySceneColorization" target="_blank">[Code]</a>
  </div>
  <div class="teaser">
    <img src="https://cdn.jsdelivr.net/gh/SketchyScene/CDN-for-figures@1.0/figures/siga19/teaser.png" style="width: 100%;">
    <br>
    <font size="3">Given a scene sketch, our system automatically produces a colorized cartoon image by progressively coloring foreground object instances and the background following user-specified language-based instructions.</font>
  </div>
</div>
<!-- === Home Section Ends === -->


<!-- === Overview Section Starts === -->
<div class="section">
  <div class="title">Abstract</div>
  <div class="body">
    Being natural, touchless, and fun-embracing, language-based inputs have been demonstrated effective for various tasks
    from image generation to literacy education for children.
    This paper for the first time presents a language-based system for interactive colorization of scene sketches,
    based on semantic comprehension.
    The proposed system is built upon deep neural networks trained on a large-scale repository of scene sketches
    and cartoonstyle color images with text descriptions.
    Given a scene sketch, our system allows users, via language-based instructions,
    to interactively localize and colorize specific foreground object instances
    to meet various colorization requirements in a progressive way.
    We demonstrate the effectiveness of our approach via comprehensive experimental results including alternative studies,
    comparison with the state-of-the-art methods, and generalization user studies.
    Given the unique characteristics of language-based inputs,
    we envision a combination of our interface with a traditional scribble-based interface
    for a practical multimodal colorization system, benefiting various applications.
  </div>
  <div class="link">
    <a href="http://mo-haoran.com/files/SIGA19/SketchColorization_paper_SA2019.pdf" target="_blank">[Main Paper]</a>&nbsp; &nbsp;
    <a href="http://mo-haoran.com/files/SIGA19/SketchColorization_supplementary_SA2019.pdf" target="_blank">[Supplementary]</a>&nbsp; &nbsp;
	<a href="https://github.com/SketchyScene/SketchySceneColorization" target="_blank">[Code]</a>&nbsp; &nbsp;
	<a href="http://mo-haoran.com/files/SIGA19/SA2019_SketchColorization_355.pptx" target="_blank">[Presentation]</a>
  </div>
</div>
<!-- === Overview Section Ends === -->


<!-- === Result Section Starts === -->
<div class="section">
  <div class="title">Methodology</div>
  <br>
  <div class="body">
    <p style="margin-top: 10pt; text-align:center; font-size:25px; font-weight:bold">A. &nbsp; System Overview<p>
    <img src="https://cdn.jsdelivr.net/gh/SketchyScene/CDN-for-figures@1.0/figures/siga19/system_overview.png" width="100%"></img>
    <font size="4">Our system supports two-mode interactive colorization for a given input scene sketch and text-based colorization instructions, using three models, namely, the instance matching model, foreground colorization model, and background colorization model. It is not necessary to colorize foreground objects before background regions.
    </font>

    <br>
    <br>
    <p style="margin-top: 10pt; text-align:center; font-size:25px; font-weight:bold">B.1 &nbsp;  Instance Matching Model<p>
    <img src="https://cdn.jsdelivr.net/gh/SketchyScene/CDN-for-figures@1.0/figures/siga19/instance_match_network.png" width="100%"></img>
    <font size="4">This network is trained in an end-to-end manner to obtain the binary mask (shown in (b)). In the inferring phase, the generated binary mask is fused with the instance segmentation results generated by Mask R-CNN to obtain the final results.
    </font>

    <br>
    <br>
    <p style="margin-top: 10pt; text-align:center; font-size:25px; font-weight:bold">B.2 &nbsp;  Foreground Colorization Model<p>
    <img src="https://cdn.jsdelivr.net/gh/SketchyScene/CDN-for-figures@1.0/figures/siga19/inst_color_network.png" width="100%"></img>
    <font size="4">This network is able to colorize objects from different categories. The generator has a U-Net architecture based on MRU blocks, with skip connections between mirrored layers and an embedded RMI fusion module consisting of LSTM text encoders and multimodal LSTMs (mLSTM). It is referred to as the FG-MRU-RMI network for conciseness in the paper.
    </font>

    <br>
    <br>
    <p style="margin-top: 10pt; text-align:center; font-size:25px; font-weight:bold">B.3 &nbsp;  Background Colorization Model<p>
    <img src="https://cdn.jsdelivr.net/gh/SketchyScene/CDN-for-figures@1.0/figures/siga19/bg_color_network.png" width="100%"></img>
    <font size="4">This network consists of an image encoder built on residual blocks (Res-Block), a fusion module, a two-branch decoder, and a Res-Block based convolutional discriminator. It is referred to as the BG-RES-RMI-SEG network in the paper.
    </font>
  </div>
</div>
<!-- === Result Section Ends === -->


<!-- === Result Section Starts === -->
<div class="section">
  <div class="title">Datasets</div>
  <div class="body">
    <img src="https://cdn.jsdelivr.net/gh/SketchyScene/CDN-for-figures@1.0/figures/siga19/dataset.png" width="100%"></img>
    <p style="margin-top: 20pt"></p>
    We have built three large-scale datasets for language-based scene sketch colorization:
    <ol>
        <li><i><strong>MATCHING dataset</strong></i>: including <strong>38k</strong> groups of text-based instance segmentation data for scene sketch.</li>
        <li><i><strong>FOREGROUND dataset</strong></i>: including <strong>4k</strong>  groups of text-based sketch object colorization data.</li>
        <li><i><strong>BACKGROUND dataset</strong></i>: including <strong>20k</strong>  groups of text-based background colorization data for scene sketch.</li>
    </ol>
  </div>
  <div class="link">
	<a href="https://github.com/SketchyScene/SketchySceneColorization" target="_blank">[Download]</a>
  </div>
</div>
<!-- === Result Section Ends === -->


<!-- === Result Section Starts === -->
<div class="section">
  <div class="title">Results</div>
  <div class="body">
    <img src="https://cdn.jsdelivr.net/gh/SketchyScene/CDN-for-figures@1.0/figures/siga19/results1.png" width="100%"></img>
    <p style="margin-top: 10pt"></p>
    <img src="https://cdn.jsdelivr.net/gh/SketchyScene/CDN-for-figures@1.0/figures/siga19/results2.png" width="100%"></img>
    <p style="margin-top: 20pt"></p>
    <font size="3">*For more results, please see main paper and the supplementary material.
    </font>

  </div>
</div>
<!-- === Result Section Ends === -->


<!-- === Result Section Starts === -->
<div class="section">
  <div class="title">Fast Forward Video</div>
  <br>
  <div class="body">
    <p style="margin-top: 20pt"><a name="demo"></a></p>
    <!-- Adjust the frame size based on the demo (EVERY project differs). -->
    <div style="position: relative; padding-top: 50%; margin: 20pt 0; text-align: center;">
      <iframe src="https://www.youtube.com/embed/nC7JBPNLRec" frameborder=0
              style="position: absolute; top: 1%; left: 5%; width: 90%; height: 100%;"
              allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
              allowfullscreen></iframe>
    </div>
  </div>
</div>
<!-- === Result Section Ends === -->


<!-- === Reference Section Starts === -->
<div class="section">
  <div class="bibtex">BibTeX</div>
<pre>
@article{zouSA2019sketchcolorization,
    title   = {Language-based Colorization of Scene Sketches},
    author  = {Zou, Changqing and Mo, Haoran and Gao, Chengying and Du, Ruofei and Fu, Hongbo},
    journal = {ACM Transactions on Graphics (Proceedings of ACM SIGGRAPH Asia 2019)},
    year    = {2019},
    volume  = 38,
    number  = 6,
    pages   = {233:1--233:16}
}
</pre>

  <br>
  <div class="bibtex">Related Work</div>
  <div class="citation">
    <div class="comment">
      Changqing Zou, Qian Yu, Ruofei Du, Haoran Mo, Yi-Zhe Song, Tao Xiang, Chengying Gao, Baoquan Chen and Hao Zhang.
      <strong>SketchyScene: Richly-Annotated Scene Sketches</strong>. ECCV, 2018.
      [<a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Changqing_Zou_SketchyScene_Richly-Annotated_Scene_ECCV_2018_paper.pdf">Paper</a>]
      [<a href="https://sketchyscene.github.io/SketchyScene/">Webpage</a>]
      [<a href="https://github.com/SketchyScene/SketchyScene">Code</a>]<br><br>
    </div>

    <div class="comment">
      Jianbo Chen, Yelong Shen, Jianfeng Gao, Jingjing Liu and Xiaodong Liu. <strong>Language-Based Image Editing with Recurrent Attentive Models</strong>. CVPR, 2018. [<a href="https://arxiv.org/pdf/1711.06288.pdf">Paper</a>][<a href="https://github.com/Jianbo-Lab/LBIE">Code</a>]<br><br>
    </div>

    <div class="comment">
      Wengling Chen and James Hays. <strong>SketchyGAN: Towards Diverse and Realistic Sketch to Image Synthesis</strong>. CVPR, 2018. [<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_SketchyGAN_Towards_Diverse_CVPR_2018_paper.pdf">Paper</a>][<a href="https://github.com/wchen342/SketchyGAN">Code</a>]<br><br>
    </div>

    <div class="comment">
      Chenxi Liu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu and Alan Yuille. <strong>Recurrent Multimodal Interaction for Referring Image Segmentation</strong>. ICCV, 2017. [<a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Liu_Recurrent_Multimodal_Interaction_ICCV_2017_paper.pdf">Paper</a>][<a href="https://github.com/chenxi116/TF-phrasecut-public">Code</a>]<br><br>
    </div>
  </div>
</div>
<!-- === Reference Section Ends === -->


</body>
</html>
